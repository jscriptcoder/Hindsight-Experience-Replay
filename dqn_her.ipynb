{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import gym.spaces\n",
    "import random\n",
    "import numpy as np\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as mpl\n",
    "from collections import deque\n",
    "from common.replay_buffer import ReplayBuffer\n",
    "from common.utils import make_experience, from_experience\n",
    "from common.device import device\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitFlipEnv:\n",
    "    def __init__(self, bits):\n",
    "        self.bits = bits\n",
    "        self.state = np.zeros(bits)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.random.randint(0, 2, size=self.bits)\n",
    "        return np.copy(self.state)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.state[action] = 1 - self.state[action]\n",
    "        return np.copy(self.state), None, None, None\n",
    "\n",
    "    def render(self):\n",
    "        print(\"State: {}\".format(self.state.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "# BITS = 50\n",
    "# env = BitFlipEnv(BITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e6)\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.98\n",
    "TAU = 0.95\n",
    "EPOCHS = 200\n",
    "CYCLES = 50\n",
    "EPISODES = 16\n",
    "MAX_STEPS = 1000\n",
    "FUTURE_K = 4\n",
    "OPTIMS = 50\n",
    "STATE_SIZE = env.observation_space.shape[0] # env.observation_space.shape[0] * 2\n",
    "ACTION_SIZE = env.action_space.n\n",
    "LR = 0.001\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.0\n",
    "EPS_DECAY = 0.95\n",
    "ENV_SOLVED = 200\n",
    "TIMES_SOLVED = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingQNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(state_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "        )\n",
    "        \n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_size),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "            \n",
    "    def forward(self, state):\n",
    "        x = self.features(state)\n",
    "        advantage = self.advantage(x)\n",
    "        value = self.value(x)\n",
    "        \n",
    "        return value + advantage  - advantage.mean(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.qn_local = DuelingQNetwork(STATE_SIZE, \n",
    "                                        ACTION_SIZE).to(device)\n",
    "        \n",
    "        self.qn_target = DuelingQNetwork(STATE_SIZE, \n",
    "                                         ACTION_SIZE).to(device)\n",
    "        \n",
    "        self.soft_update(1.)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.qn_local.parameters(), lr=LR)\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        self.qn_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qn_local(state)\n",
    "        self.qn_local.train()\n",
    "        \n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(ACTION_SIZE))\n",
    "\n",
    "    def optimize(self):\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            return self.learn(experiences)\n",
    "            \n",
    "    def learn(self, experiences):\n",
    "        (states, \n",
    "         actions, \n",
    "         rewards, \n",
    "         next_states, \n",
    "         dones) = from_experience(experiences)\n",
    "        \n",
    "        best_action = self.qn_local(next_states).argmax(-1, keepdim=True)\n",
    "        max_q = self.qn_target(next_states).detach().gather(-1, best_action)\n",
    "        \n",
    "        q_targets = rewards + (GAMMA * max_q * (1 - dones))\n",
    "        q_expected = self.qn_local(states).gather(-1, actions)\n",
    "\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "    def soft_update(self, tau):\n",
    "        for target_param, local_param in zip(self.qn_target.parameters(), \n",
    "                                             self.qn_local.parameters()):\n",
    "            target_param.data.copy_(tau * local_param + (1.0 - tau) * target_param)\n",
    "    \n",
    "    def add_experience(self, state, action, reward, next_state, done):\n",
    "        experience = make_experience(state, \n",
    "                                     action, \n",
    "                                     reward, \n",
    "                                     next_state, \n",
    "                                     done)\n",
    "        self.memory.add(experience)\n",
    "    \n",
    "    def make_goal(self):\n",
    "        return np.array([0., 0., 0., 0., 0., 0., 1., 1.])\n",
    "    \n",
    "    def compute_reward(self, state, goal, eps=0.001):\n",
    "        done = np.sum(np.abs(goal - state)) < eps\n",
    "        return 0. if done else -1., done\n",
    "    \n",
    "    def eval_episode(self):\n",
    "        total_reward = 0\n",
    "        goal = self.make_goal()\n",
    "        \n",
    "        for _ in range(TIMES_SOLVED):\n",
    "            state = env.reset()\n",
    "            \n",
    "            for step in range(MAX_STEPS):\n",
    "#                 action = self.act(np.concatenate([state, goal]))\n",
    "                action = self.act(state)\n",
    "                state, reward, done, _ = env.step(action)\n",
    "\n",
    "                total_reward += reward\n",
    "    \n",
    "                if done: break\n",
    "                \n",
    "        return total_reward / TIMES_SOLVED\n",
    "    \n",
    "    def train(self):\n",
    "        eps = EPS_START\n",
    "        \n",
    "        for epoch in range(1, EPOCHS+1):\n",
    "            \n",
    "            success = 0\n",
    "            \n",
    "            for cycle in range(CYCLES):\n",
    "            \n",
    "                for episode in range(EPISODES):\n",
    "\n",
    "                    trajectory = []\n",
    "                    state = env.reset()\n",
    "                    goal = self.make_goal()\n",
    "\n",
    "                    score = 0\n",
    "\n",
    "                    for step in range(MAX_STEPS):\n",
    "#                         action = self.act(np.concatenate([state, goal]), eps)\n",
    "                        action = self.act(state, eps)\n",
    "                        next_state, env_reward, env_done, _ = env.step(action)\n",
    "#                         reward, done = self.compute_reward(next_state, goal)\n",
    "\n",
    "#                         trajectory.append(make_experience(state, action, env_reward, next_state, done))\n",
    "                        trajectory.append(make_experience(state, action, env_reward, next_state, env_done))\n",
    "\n",
    "                        state = next_state\n",
    "\n",
    "#                         if done: success += 1\n",
    "                        \n",
    "                        if env_done: break\n",
    "\n",
    "                    steps_taken = len(trajectory)\n",
    "\n",
    "                    for t in range(steps_taken):\n",
    "                        state, action, reward, next_state, done = trajectory[t]\n",
    "                        \n",
    "#                         self.add_experience(np.concatenate([state, goal]), \n",
    "#                                             action, \n",
    "#                                             reward, \n",
    "#                                             np.concatenate([next_state, goal]), \n",
    "#                                             done)\n",
    "\n",
    "                        self.add_experience(state, \n",
    "                                            action, \n",
    "                                            reward, \n",
    "                                            next_state, \n",
    "                                            done)\n",
    "\n",
    "#                         for _ in range(FUTURE_K):\n",
    "#                             future = np.random.randint(t, steps_taken)\n",
    "#                             achieved_goal = trajectory[future].next_state\n",
    "#                             reward, done = self.compute_reward(next_state, achieved_goal)\n",
    "                            \n",
    "#                             self.add_experience(np.concatenate([state, achieved_goal]), \n",
    "#                                                 action, \n",
    "#                                                 reward, \n",
    "#                                                 np.concatenate([next_state, achieved_goal]), \n",
    "#                                                 done)\n",
    "                            \n",
    "                # End Episode\n",
    "\n",
    "                for _ in range(OPTIMS):\n",
    "                    loss = self.optimize()\n",
    "\n",
    "                self.soft_update(TAU)\n",
    "                \n",
    "            # End Cycle\n",
    "            \n",
    "            success_rate = success / (EPISODES * CYCLES)\n",
    "            \n",
    "            print('\\rEpoch {}\\tExploration: {:.2f}%\\tSuccess Rate: {:.2}\\tLast Loss: {:.4f}'.format(\n",
    "                epoch, \n",
    "                100*eps, \n",
    "                success_rate, \n",
    "                loss\n",
    "            ), end='')\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                print('\\nRunning evaluation...')\n",
    "\n",
    "                mean_score = self.eval_episode()\n",
    "\n",
    "                if mean_score >= ENV_SOLVED:\n",
    "                    print('Environment solved {} times consecutively!'.format(TIMES_SOLVED))\n",
    "                    print('Avg score: {:.3f}'.format(mean_score))\n",
    "                    break\n",
    "                else:\n",
    "                    print('No success. Avg score: {:.3f}'.format(mean_score))\n",
    "            \n",
    "            eps = max(EPS_END, EPS_DECAY*eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\tExploration: 81.45%\tSuccess Rate: 0.0\tLast Loss: 3.957032\n",
      "Running evaluation...\n",
      "No success. Avg score: -197.330\n",
      "Epoch 10\tExploration: 63.02%\tSuccess Rate: 0.0\tLast Loss: 9.26015\n",
      "Running evaluation...\n",
      "No success. Avg score: -118.832\n",
      "Epoch 15\tExploration: 48.77%\tSuccess Rate: 0.0\tLast Loss: 20.6801\n",
      "Running evaluation...\n",
      "No success. Avg score: -11.940\n",
      "Epoch 20\tExploration: 37.74%\tSuccess Rate: 0.0\tLast Loss: 7.44047\n",
      "Running evaluation...\n",
      "Environment solved 100 times consecutively!\n",
      "Avg score: 208.168\n"
     ]
    }
   ],
   "source": [
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
