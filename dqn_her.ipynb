{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import gym.spaces\n",
    "import random\n",
    "import numpy as np\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as mpl\n",
    "from collections import deque\n",
    "from common.replay_buffer import ReplayBuffer\n",
    "from common.utils import make_experience, from_experience\n",
    "from common.device import device\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitFlipEnv:\n",
    "    def __init__(self, bits):\n",
    "        self.bits = bits\n",
    "        self.state = np.zeros(bits)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.random.randint(0, 2, size=self.bits)\n",
    "        return np.copy(self.state)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.state[action] = 1 - self.state[action]\n",
    "        return np.copy(self.state), None, None, None\n",
    "\n",
    "    def render(self):\n",
    "        print(\"State: {}\".format(self.state.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rocket_lander_gym\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "# BITS = 50\n",
    "# env = BitFlipEnv(BITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e6)\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.98\n",
    "TAU = 1. # 0.95\n",
    "EPOCHS = 50\n",
    "CYCLES = 50\n",
    "EPISODES = 16\n",
    "MAX_STEPS = 1000\n",
    "FUTURE_K = 4\n",
    "OPTIMS = 50\n",
    "STATE_SIZE = env.observation_space.shape[0] * 2\n",
    "ACTION_SIZE = env.action_space.n\n",
    "LR = 0.001\n",
    "EPS_START = 0.2\n",
    "EPS_END = 0.0\n",
    "EPS_DECAY = 0.95\n",
    "ENV_SOLVED = 200\n",
    "TIMES_SOLVED = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingQNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(state_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "        )\n",
    "        \n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_size),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "            \n",
    "    def forward(self, state):\n",
    "        x = self.features(state)\n",
    "        advantage = self.advantage(x)\n",
    "        value = self.value(x)\n",
    "        \n",
    "        return value + advantage  - advantage.mean(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.qn_local = DuelingQNetwork(STATE_SIZE, \n",
    "                                        ACTION_SIZE).to(device)\n",
    "        \n",
    "        self.qn_target = DuelingQNetwork(STATE_SIZE, \n",
    "                                         ACTION_SIZE).to(device)\n",
    "        \n",
    "        self.soft_update(1.)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.qn_local.parameters(), lr=LR)\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
    "\n",
    "    def act(self, state, eps=0., use_target=False):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        model = self.qn_target if use_target else self.qn_local\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = model(state)\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(ACTION_SIZE))\n",
    "\n",
    "    def optimize(self):\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            return self.learn(experiences)\n",
    "            \n",
    "    def learn(self, experiences):\n",
    "        (states, \n",
    "         actions, \n",
    "         rewards, \n",
    "         next_states, \n",
    "         dones) = from_experience(experiences)\n",
    "        \n",
    "        best_action = self.qn_local(next_states).argmax(-1, keepdim=True)\n",
    "        max_q = self.qn_target(next_states).detach().gather(-1, best_action)\n",
    "        \n",
    "        q_targets = rewards + (GAMMA * max_q * (1 - dones))\n",
    "        q_expected = self.qn_local(states).gather(-1, actions)\n",
    "\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "    def soft_update(self, tau):\n",
    "        for target_param, local_param in zip(self.qn_target.parameters(), \n",
    "                                             self.qn_local.parameters()):\n",
    "            target_param.data.copy_(tau * local_param + (1.0 - tau) * target_param)\n",
    "    \n",
    "    def add_experience(self, state, action, reward, next_state, done):\n",
    "        experience = make_experience(state, \n",
    "                                     action, \n",
    "                                     reward, \n",
    "                                     next_state, \n",
    "                                     done)\n",
    "        self.memory.add(experience)\n",
    "    \n",
    "    def make_goal(self):\n",
    "        return np.array([0., 0., 0., 0., 0., 0., 1., 1.])\n",
    "    \n",
    "    def compute_reward(self, state, goal, eps=0.1):\n",
    "        done = np.sum(np.abs(goal - state)) < eps\n",
    "        return 0. if done else -1., done\n",
    "    \n",
    "    def eval_episode(self, use_target=False, render=False):\n",
    "        total_reward = 0\n",
    "        goal = self.make_goal()\n",
    "        \n",
    "        for t in range(TIMES_SOLVED):\n",
    "            state = env.reset()\n",
    "            \n",
    "            for step in range(MAX_STEPS):\n",
    "                \n",
    "                if render and t == TIMES_SOLVED-1: env.render()\n",
    "                    \n",
    "                action = self.act(np.concatenate([state, goal]), use_target=use_target)\n",
    "                # action = self.act(state)\n",
    "                state, reward, done, _ = env.step(action)\n",
    "\n",
    "                total_reward += reward\n",
    "    \n",
    "                if done: break\n",
    "            \n",
    "            if render: env.close()\n",
    "                \n",
    "        return total_reward / TIMES_SOLVED\n",
    "    \n",
    "    def train(self):\n",
    "        eps = EPS_START\n",
    "        \n",
    "        for epoch in range(1, EPOCHS+1):\n",
    "            \n",
    "            success = 0\n",
    "            \n",
    "            for cycle in range(CYCLES):\n",
    "            \n",
    "                for episode in range(EPISODES):\n",
    "\n",
    "                    trajectory = []\n",
    "                    state = env.reset()\n",
    "                    goal = self.make_goal()\n",
    "\n",
    "                    score = 0\n",
    "\n",
    "                    for step in range(MAX_STEPS):\n",
    "                        action = self.act(np.concatenate([state, goal]), eps)\n",
    "                        # action = self.act(state, eps)\n",
    "                        next_state, env_reward, env_done, _ = env.step(action)\n",
    "                        reward, done = self.compute_reward(next_state, goal)\n",
    "\n",
    "                        trajectory.append(make_experience(state, action, env_reward, next_state, done))\n",
    "                        # trajectory.append(make_experience(state, action, env_reward, next_state, env_done))\n",
    "\n",
    "                        state = next_state\n",
    "\n",
    "                        if done: success += 1\n",
    "                        \n",
    "                        if env_done: break\n",
    "\n",
    "                    steps_taken = len(trajectory)\n",
    "\n",
    "                    for t in range(steps_taken):\n",
    "                        state, action, reward, next_state, done = trajectory[t]\n",
    "                        \n",
    "                        self.add_experience(np.concatenate([state, goal]), \n",
    "                                            action, \n",
    "                                            reward, \n",
    "                                            np.concatenate([next_state, goal]), \n",
    "                                            done)\n",
    "\n",
    "                        # self.add_experience(state, \n",
    "                        #                     action, \n",
    "                        #                     reward, \n",
    "                        #                     next_state, \n",
    "                        #                     done)\n",
    "\n",
    "                        for _ in range(FUTURE_K):\n",
    "                            future = np.random.randint(t, steps_taken)\n",
    "                            achieved_goal = trajectory[future].next_state\n",
    "                            reward, done = self.compute_reward(next_state, achieved_goal)\n",
    "                            \n",
    "                            self.add_experience(np.concatenate([state, achieved_goal]), \n",
    "                                                action, \n",
    "                                                reward, \n",
    "                                                np.concatenate([next_state, achieved_goal]), \n",
    "                                                done)\n",
    "                            \n",
    "                # End Episode\n",
    "\n",
    "                for _ in range(OPTIMS):\n",
    "                    loss = self.optimize()\n",
    "\n",
    "                self.soft_update(TAU)\n",
    "                \n",
    "            # End Cycle\n",
    "            \n",
    "            success_rate = success / (EPISODES * CYCLES)\n",
    "            \n",
    "            print('\\rEpoch {}\\tExploration: {:.2f}%\\tSuccess Rate: {:.2}\\tLast Loss: {:.4f}'.format(\n",
    "                epoch, \n",
    "                100*eps, \n",
    "                success_rate, \n",
    "                loss\n",
    "            ), end='')\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                print('\\nRunning evaluation...')\n",
    "\n",
    "                mean_score = self.eval_episode(use_target=False, render=True)\n",
    "\n",
    "                if mean_score >= ENV_SOLVED:\n",
    "                    print('Environment solved {} times consecutively!'.format(TIMES_SOLVED))\n",
    "                    print('Avg score: {:.3f}'.format(mean_score))\n",
    "                    break\n",
    "                else:\n",
    "                    print('No success. Avg score: {:.3f}'.format(mean_score))\n",
    "            \n",
    "            eps = max(EPS_END, EPS_DECAY*eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\tExploration: 16.29%\tSuccess Rate: 0.0\tLast Loss: 1.28671546\n",
      "Running evaluation...\n",
      "No success. Avg score: -123.666\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3d92c9b1ff97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-36897fcc3bd0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                         \u001b[0;31m# action = self.act(state, eps)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-36897fcc3bd0>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state, eps, use_target)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mACTION_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-build/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m     \"\"\"\n\u001b[0;32m-> 1186\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-build/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "goal = agent.make_goal()\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    action = agent.act(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    if done:\n",
    "        print(\"Simulation done.\")\n",
    "        print(state)\n",
    "        print(np.sum(np.abs(goal - state)) < 0.2)\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
